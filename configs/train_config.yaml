model_name: deepseek-ai/DeepSeek-Coder-V2-Lite-Base
public_models_yaml: LoRA/configs/public_models.yaml
stage1_dataset: LoRA/labeled/final_general_stage1.jsonl
stage2_dataset: LoRA/labeled/final_minecraft_primary.jsonl
output_dir: LoRA/output
deepspeed_config: LoRA/deepspeed/zero3.json
max_seq_len: 2048
per_device_train_batch_size: 1
gradient_accumulation_steps: 16
num_train_epochs_stage1: 1.0
num_train_epochs_stage2: 1.0
learning_rate_stage1: 0.0002
learning_rate_stage2: 0.00015
lora_r: 64
lora_alpha: 128
lora_dropout: 0.05
target_modules: q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj
save_steps: 500
eval_steps: 500
logging_steps: 20
seed: 42
packing: true
